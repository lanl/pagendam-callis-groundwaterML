{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3207889-22af-47c3-9836-1420b67a864f",
   "metadata": {},
   "source": [
    "# Test Run-Through of the Training Routine\n",
    "\n",
    "This notebook does a run-through of the training routine, using the methodology from `../model_and_training_fns/training_routine.py`, supported by `../model_and_training_fns/training_fns.py`.\n",
    "\n",
    "Additionally, it describes issues that arose regarding the RMSProp Optimizer and the Distribution Loss function. It also attempts to debug why the models aren't learning effectively by performing checks on the training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0233d37f-6eb8-4cdd-96fb-dc8804167471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../model_and_training_fns/'))\n",
    "from model_definition import CNN\n",
    "from dataloader import GW_DataSet, make_dataloader\n",
    "import training_fns as pyt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8b7944-82f4-478d-92dc-2091472214db",
   "metadata": {},
   "source": [
    "## Initiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b21c39-3b69-43c6-ad61-952a97afaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(local_effects_kernel=3,\n",
    "            local_effects_neurons=8,\n",
    "            local_effects_bias=True,\n",
    "            local_effects_depth=3,\n",
    "            activation=nn.ReLU,\n",
    "            dense_neurons=32,\n",
    "            dense_bias=True,\n",
    "            dense_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ca268-318c-4311-b6c8-cca8bcbc649c",
   "metadata": {},
   "source": [
    "## About RMSProp As an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b644f-54f8-49dc-9643-aba6906b6ed6",
   "metadata": {},
   "source": [
    "From [*A log-additive neural model for spatio-temporal prediction of groundwater levels*](https://doi.org/10.1016/j.spasta.2023.100740) by Pagendam et. al.:\n",
    "\n",
    "> [...] we update the parameters of the model using the RMSProp method [...] The parameter $\\beta$ is a hyper-parameter chosen by the user, and in this implementation we used the default value (in the Keras API) of 0.9. Model training was performed using an initial learning rate of $\\gamma = 1e -6$, and all model parameters were initialised randomly according to the Glorot uniform distribution method (Glorot and Bengio, 2010).\n",
    "\n",
    "Keras Documentation for RMSProp: [https://keras.io/api/optimizers/rmsprop/](https://keras.io/api/optimizers/rmsprop/)\n",
    "\n",
    "An Explanation of the Keras RMSProp: [https://python.plainenglish.io/keras-optimizers-explained-rmsprop-93febeaba374](https://python.plainenglish.io/keras-optimizers-explained-rmsprop-93febeaba374)\n",
    "\n",
    "PyTorch Documentation for RMSProp: [https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) \n",
    "\n",
    "*In conclusion...*\n",
    "\n",
    "Keras `rho` == Pytorch $\\alpha$ == Pytorch `alpha` == Pagendam $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe09ea8-f7da-4d7f-97cd-da8ed32b8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                                lr=1e-6, \n",
    "                                alpha=0.9, \n",
    "                                eps=1e-08, \n",
    "                                weight_decay=0, \n",
    "                                momentum=0, \n",
    "                                centered=False, \n",
    "                                foreach=None, \n",
    "                                maximize=False, \n",
    "                                differentiable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9bbba-2975-4e84-aaa0-624cfd8901e6",
   "metadata": {},
   "source": [
    "## Setting Up the DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0bd0236-75ab-4a55-8cc8-b8da9597bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GW_DataSet(os.path.join('..', '..', 'NPYdata', 'train'))\n",
    "val_dataset = GW_DataSet(os.path.join('..', '..', 'NPYdata', 'tune'))\n",
    "test_dataset = GW_DataSet(os.path.join('..', '..', 'NPYdata', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d28a1f-10cc-4c82-95d2-87917923b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_summary_dict = {\n",
    "                        \"train_loss\": [],\n",
    "                        \"val_loss\": [],\n",
    "                        \"epoch_time\": []\n",
    "                        }\n",
    "train_samples_dict = {\n",
    "                        \"epoch\": [],\n",
    "                        \"batch\": [],\n",
    "                        \"truth\": [],\n",
    "                        \"mu1_prediction\": [],\n",
    "                        \"mu2_prediction\": [],\n",
    "                        \"log_sigma1_prediction\": [],\n",
    "                        \"log_sigma2_prediction\": [],\n",
    "                        \"loss\": []\n",
    "                        }\n",
    "val_samples_dict = {\n",
    "                        \"epoch\": [],\n",
    "                        \"batch\": [],\n",
    "                        \"truth\": [],\n",
    "                        \"mu1_prediction\": [],\n",
    "                        \"mu2_prediction\": [],\n",
    "                        \"log_sigma1_prediction\": [],\n",
    "                        \"log_sigma2_prediction\": [],\n",
    "                        \"loss\": []\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acb4bf7-dd47-4424-a4f6-e493a806bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_batches = 10\n",
    "val_batches = 5\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size, train_batches)\n",
    "val_dataloader = make_dataloader(val_dataset, batch_size, val_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4112201-9721-4aba-9d97-7144ee4d4c5a",
   "metadata": {},
   "source": [
    "## About the Distrobution Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe6032-6afc-4fac-bfb2-0936f281c855",
   "metadata": {},
   "source": [
    "### From the paper\n",
    "\n",
    "From [*A log-additive neural model for spatio-temporal prediction of groundwater levels*](https://doi.org/10.1016/j.spasta.2023.100740) by Pagendam et. al.:\n",
    "\n",
    ">Specifically, our observations of groundwater depth are modelled as having a log-normal density:\n",
    ">$$ f(y_i | x_{1, i}, x_{2, i}, \\theta_1, \\theta_2) = (2*\\pi)^{-\\frac{1}{2}} * (\\sigma(x_{1, i}, x_{2, i}) * y_i)^{-1} * \\exp({ -1 * \\frac{(log(y_i) - \\mu(x_{1, i}, x_{2, i}))^2}{2 \\sigma^2(x_{1, i}, x_{2, i})}}) $$\n",
    ">where $\\mu(x_{1, i}, x_{2, i}) = \\mu_1(x_{1, i}) + \\mu_2(x_{2, i})$ and $\\sigma^2(x_{1, i}, x_{2, i}) = \\sigma^2_1(x_{1, i}) + \\sigma^2_2(x_{2, i})$. As a result of this simple summation of means and variances, we refer to and as log-additive components of our model: outputs of the two architectures (i.e. the means and variances on Gaussian distributions on the log-scale) that can be added together to give the first and second moments of the predictive distribution on the log-scale.\n",
    ">\n",
    ">[...]\n",
    ">\n",
    ">The loss function over the batch can be written as:\n",
    ">$$ \\ell(y_i | x_{1, i}, x_{2, i}, \\theta_1, \\theta_2) = - \\Sigma_{i=1}^{batchsize} log( f(y_i | x_{1, i}, x_{2, i}, \\theta_1, \\theta_2) ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e6ba6-616e-434c-9ba9-85c065cd7ebc",
   "metadata": {},
   "source": [
    "### From discussions with Pagendam\n",
    "\n",
    "Pagendam describes implementing the loss function in `R` as follows:\n",
    "\n",
    "`ll <- -0.5*K$square((mu - log(y_true))/(sigma)) - K$log(sigma) - log(y_true) - 0.5*log(2*pi)`\n",
    "\n",
    "$$\\ell = -0.5 * (\\frac{\\mu - log(y_{true})}{\\sigma})^2 - log(\\sigma) - log(y_{true}) - 0.5*log(2*\\pi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e773b9e-eb5a-4974-aec5-6a3205b157b0",
   "metadata": {},
   "source": [
    "### Skylar's Naive Implementation\n",
    "\n",
    "The model outputs as predictions $\\mu_1$, $\\mu_2$, $log(\\sigma_1)$,and $log(\\sigma_1)$. The first step is to consolidate values to $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "$$ \\sigma_1 = e^{log(\\sigma_1)}$$\n",
    "\n",
    "$$ \\sigma_2 = e^{log(\\sigma_2)}$$\n",
    "\n",
    "$$ \\mu = \\mu_1 + \\mu_2 $$\n",
    "\n",
    "$$ \\sigma^2 = \\sigma_1^2 + \\sigma_2^2 $$\n",
    "\n",
    "Skylar then implemented the loss function in `PyTorch` as:\n",
    "\n",
    "`f = ((2*math.pi)**(-1/2)) * ((torch.sqrt(sigma_sqr) * y)**(-1)) * torch.exp((-1) * ((torch.log(y) - mu)**2) * ((2 * sigma_sqr)**(-1)))`\n",
    "\n",
    "`torch.log(f)`\n",
    "\n",
    "Or equivalently,\n",
    "\n",
    "$$ f = (2*\\pi)^{-\\frac{1}{2}} * (\\sqrt{\\sigma^2} * y_{true})^{-1} * \\exp(-1 * (log(y_{true}) - \\mu)^2 * (2 * \\sigma^2)^{-1} ) $$\n",
    "\n",
    "$$ \\ell = log(f) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ceaf2-8137-4482-b5d0-89bdef0813fc",
   "metadata": {},
   "source": [
    "#### Skylar's Attempt at Log Arithmatic\n",
    "\n",
    "To convince themselves that their naive implementation was indeed equivalent to Pagendam's implementation, they did this bought of log arithmatic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f31a4-b691-4107-8868-fe90bc389916",
   "metadata": {},
   "source": [
    "$$ log(f) = log\\left( \\color{lime}{(2*\\pi)^{-\\frac{1}{2}}} \\color{white}{*} \\color{cyan}{(\\sqrt{\\sigma^2} * y_{true})^{-1}} \\color{white}{*} \\color{yellow}{e^{ -1 * \\frac{(log(y) - \\mu)^2}{2 \\sigma^2}}} \\color{white}\\right)$$\n",
    "\n",
    "$$ log(f) = \\color{lime}{log\\left( (2*\\pi)^{-\\frac{1}{2}} \\right)} \\color{white}{+} \\color{cyan}{log\\left((\\sqrt{\\sigma^2} * y_{true})^{-1} \\right)} \\color{white}{+} \\color{yellow}{log\\left(e^{ -1 * \\frac{(log(y) - \\mu)^2}{2 \\sigma^2}} \\right)}$$\n",
    "\n",
    "$$ log(f) = \\color{lime}{-\\frac{1}{2}log\\left( (2*\\pi) \\right)} \\color{white}{+} \\color{cyan}{(-1)*log\\left((\\sqrt{\\sigma^2} * y_{true}) \\right)} \\color{white}{+} \\color{yellow}{(-1) * \\frac{(log(y) - \\mu)^2}{2 \\sigma^2}}$$\n",
    "\n",
    "$$ log(f) = \\color{lime}{-\\frac{1}{2} * log\\left( (2*\\pi) \\right)} \\color{white}{+} \\color{cyan}{(-1)*log\\left(\\sigma\\right) + (-1)*log\\left( y_{true} \\right)} \\color{white}{+} \\color{yellow}{(-1) * \\frac{(log(y_{true}) - \\mu)^2}{2 * \\sigma^2}}$$\n",
    "\n",
    "$$ log(f) = \\color{lime}{-0.5* log(2\\pi)} \\color{cyan}{- log(\\sigma) - log( y_{true})} \\color{yellow}{- \\frac{(log(y_{true}) - \\mu)^2}{2 \\sigma^2}}$$\n",
    "\n",
    "$$ log(f) = \\color{lime}{-0.5* log(2\\pi)} \\color{cyan}{- log(\\sigma) - log( y_{true})}  \\color{yellow}{-(\\frac{1}{2}) \\frac{(-1)^2(\\mu - log(y_{true}))^2}{\\sigma^2}}$$\n",
    "\n",
    "$$ log(f) = \\color{lime}{-0.5* log(2\\pi)} \\color{cyan}{- log(\\sigma) - log( y_{true})}  \\color{yellow}{-0.5 *  \\left(\\frac{\\mu - log(y_{true}}{\\sigma}\\right)^2}$$\n",
    "\n",
    "which matches Pagendam's implementation:\n",
    "\n",
    "$$\\ell \\ell = \\color{yellow}{-0.5 * (\\frac{\\mu - log(y_{true})}{\\sigma})^2} \\color{cyan}{- log(\\sigma) - log(y_{true})} \\color{lime}{- 0.5*log(2*\\pi) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37b292-7438-4714-975e-96a4ef78c8de",
   "metadata": {},
   "source": [
    "Note that the loss \n",
    "\n",
    "$$\\ell = - \\Sigma_{i=1}^{batchsize} log(f)$$\n",
    "\n",
    "so we need to multiply by $-1$ before we pass the loss to the next step in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e7200-6aac-4e5d-9422-0ba26c77c8f5",
   "metadata": {},
   "source": [
    "### Loss Function in `PyTorch`\n",
    "\n",
    "These functions are from `training_fns.py` and could be imported as `pyt.Distribution_Loss` and `pyt.Distribution_Loss_v2` respectively\n",
    "\n",
    "`Distribution_Loss` is the naive implementation of the loss function described in Pagendam et. al.\n",
    "\n",
    "`Distribution_Loss_v2` is the implementation that matches Pagendam's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9db96a-c997-4ffb-b13b-602fb2069003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution_Loss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Distribution_Loss, self).__init__()\n",
    "\n",
    "\tdef forward(self, preds: tuple[float, float, float, float], truth: float):\n",
    "\t\t\"\"\"\n",
    "\t\tCustom loss for groundwater data\n",
    "\t\tArgs:\n",
    "\t\t\tpred (torch.tensor([[float, float, float, float], ...])): \n",
    "\t\t\t\tshape: (batchsize, 4)\n",
    "\t\t\t\tarray of prediction values (mu1, log_sigma1, mu2, log_sigma2) for batch of samples\n",
    "\t\t\ttruth (torch.tensor([[float], ...])): \n",
    "\t\t\t\tshape: (batchsize, 1)\n",
    "\t\t\t\tarray of GW_depth (ground truth) for batch of samples\n",
    "\t\tReturns:\n",
    "\t\t\tloss (torch.tensor([[float], ...])): \n",
    "\t\t\t\tshape: (batchsize, 1)\n",
    "\t\t\t\tarray of loss values for batch of samples\n",
    "\t\t\"\"\"\n",
    "\t\tbatchsize = np.shape(truth.cpu().detach().numpy().flatten())[0]\n",
    "\t\tloss = torch.zeros(batchsize)\n",
    "\t\tfor i in range(batchsize):\n",
    "\t\t\tmu1, log_sigma1, mu2, log_sigma2 = preds[i]\n",
    "\t\t\ty = truth[i]\n",
    "\t\t\tsigma1 = torch.exp(log_sigma1)\n",
    "\t\t\tsigma2 = torch.exp(log_sigma2)\n",
    "\t\t\tmu = mu1 + mu2\n",
    "\t\t\tsigma_sqr = (sigma1**2) + (sigma2**2)\n",
    "\t\t\tf = ((2*math.pi)**(-1/2)) * ((torch.sqrt(sigma_sqr) * y)**(-1)) * torch.exp((-1) * ((torch.log(y) - mu)**2) * ((2 * sigma_sqr)**(-1)))\n",
    "\t\t\tloss[i] = torch.log(f)\n",
    "\n",
    "\t\tloss *= -1\t\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6803c33d-aa54-4b06-9d43-87a3db5ef6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution_Loss_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Distribution_Loss_v2, self).__init__()\n",
    "    \n",
    "    def forward(self, preds: tuple[float, float, float, float], truth: float):\n",
    "        \"\"\"\n",
    "        Custom loss for groundwater data\n",
    "            Args:\n",
    "                pred (torch.tensor([[float, float, float, float], ...])): \n",
    "                shape: (batchsize, 4)\n",
    "                        array of prediction values (mu1, log_sigma1, mu2, log_sigma2) for batch of samples\n",
    "                truth (torch.tensor([[float], ...])): \n",
    "                    shape: (batchsize, 1)\n",
    "                    array of GW_depth (ground truth) for batch of samples\n",
    "            Returns:\n",
    "            loss (torch.tensor([[float], ...])): \n",
    "                shape: (batchsize, 1)\n",
    "                array of loss values for batch of samples\n",
    "        \"\"\"\n",
    "        batchsize = np.shape(truth.cpu().detach().numpy().flatten())[0]\n",
    "        loss = torch.zeros(batchsize)\n",
    "        for i in range(batchsize):\n",
    "            mu1, log_sigma1, mu2, log_sigma2 = preds[i]\n",
    "            y = truth[i]\n",
    "            sigma1 = torch.exp(log_sigma1)\n",
    "            sigma2 = torch.exp(log_sigma2)\n",
    "            mu = mu1 + mu2\n",
    "            sigma = torch.sqrt((sigma1**2) + (sigma2**2))\n",
    "            loss[i] = -0.5*math.log(2*math.pi) - torch.log(sigma) - torch.log(y) -0.5*(( (mu - torch.log(y)) / sigma )**2)\n",
    "            loss[i] *= -1\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d347b47-f77e-4043-a2b8-df768490deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = Distribution_Loss_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799feb73-d73c-4156-a394-dbd947bba8bc",
   "metadata": {},
   "source": [
    "## Check if Weights are Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3afe4-e607-43ff-a781-d0f12b26bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialweights = model.sub1.precipitation_branch.conv_layers[0].weight\n",
    "for traindata in train_dataloader:\n",
    "    truth, pred, train_loss = pyt.train_datastep(traindata, \n",
    "                                                model,\n",
    "                                                optimizer,\n",
    "                                                loss_fn,\n",
    "                                                device)\n",
    "finalweights = model.sub1.precipitation_branch.conv_layers[0].weight\n",
    "torch.all(initialweights == finalweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c2fdd-dfda-4d58-89b4-8e8e62459da3",
   "metadata": {},
   "source": [
    "... the weights are not updating. I'm not sure why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e5ef3-1ff8-4a62-8018-792d925cefa4",
   "metadata": {},
   "source": [
    "## Check the `Train Epoch` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87011426-b97d-4483-8ef5-6d2d6f13bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict, train_sample_dict, val_sample_dict = pyt.train_epoch(train_dataloader,\n",
    "                                                                    val_dataloader, \n",
    "                                                                    model,\n",
    "                                                                    optimizer,\n",
    "                                                                    loss_fn,\n",
    "                                                                    train_val_summary_dict,\n",
    "                                                                    train_samples_dict,\n",
    "                                                                    val_samples_dict,\n",
    "                                                                    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c856d60-a65e-424b-90ee-b18bc4e21f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sample_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0e872-7ea3-4076-b659-930c37ee9e13",
   "metadata": {},
   "source": [
    "## Check Functions that Save Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72baa576-ca56-4ac1-b18b-6351941a33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_batches = 10\n",
    "val_batches = 5\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size, train_batches)\n",
    "val_dataloader = make_dataloader(val_dataset, batch_size, val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c368e-178f-42e8-9869-27778981809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainbatches = len(train_dataloader)\n",
    "valbatches = len(val_dataloader)\n",
    "trainbatch_ID = 0\n",
    "valbatch_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115b172-1fbe-4613-be84-76998ceaaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_dict(dictt: dict, batch_ID: int, truth, pred, loss):\n",
    "\t\"\"\" Function to appending sample information to a dictionary\n",
    "\t\tDictionary must be initialized with correct keys\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tdictt (dict): dictionary to append sample information to\n",
    "\t\t\tbatch_ID (int): batch ID number for samples\n",
    "\t\t\ttruth (): array of truth values for batch of samples\n",
    "\t\t\tpred (): array of prediction values for batch of samples\n",
    "\t\t\tloss (): array of loss values for batch of samples\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\tdictt (dict): dictionary with appended sample information\n",
    "\t\"\"\"\n",
    "\tbatchsize = np.shape(truth.cpu().detach().numpy().flatten())[0]\n",
    "\tfor i in range(batchsize):\n",
    "\t\tdictt[\"epoch\"].append(0) # To be easily identified later\n",
    "\t\tdictt[\"batch\"].append(batch_ID)\n",
    "\t\tdictt[\"truth\"].append(truth.cpu().detach().numpy().flatten()[i])\n",
    "\t\tmu1, log_sigma1, mu2, log_sigma2 = pred[i]\n",
    "\t\tdictt[\"mu1_prediction\"].append(mu1.cpu().detach().numpy().flatten()[0])\n",
    "\t\tdictt[\"mu2_prediction\"].append(mu2.cpu().detach().numpy().flatten()[0])\n",
    "\t\tdictt[\"log_sigma1_prediction\"].append(log_sigma1.cpu().detach().numpy().flatten()[0])\n",
    "\t\tdictt[\"log_sigma2_prediction\"].append(log_sigma2.cpu().detach().numpy().flatten()[0])\n",
    "\t\tdictt[\"loss\"].append(loss[i].cpu().detach().numpy().flatten()[0])\n",
    "\n",
    "\treturn dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f297172-852d-4d46-acb0-8c5b61fd4e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for traindata in train_dataloader:\n",
    "    print('batch:', trainbatch_ID)\n",
    "    trainbatch_ID += 1\n",
    "    model.train()\n",
    "    GW_depth = traindata['GW_depth'].to(torch.float32).unsqueeze(-1).to(device)\n",
    "    preds = model(device, traindata)\n",
    "    print('preds', preds)\n",
    "    print('truth', GW_depth)\n",
    "    loss = loss_fn(preds, GW_depth)\n",
    "    print('loss', loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "    train_sample_dict = append_to_dict(train_samples_dict, trainbatch_ID, GW_depth, preds, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890c018-a0aa-4b9a-92c7-ef7a83192c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('batchsize:', batch_size)\n",
    "print('Predictions shape:', preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfdcefd-bad0-4ee0-bc57-54602f195716",
   "metadata": {},
   "source": [
    "## Check that the batches are correctly split into samaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235141e-7b27-4c3e-8518-04cbfe728b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for traindata in train_dataloader:\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    ## Run as a batch\n",
    "    GW_depth = traindata['GW_depth'].to(torch.float32).unsqueeze(-1).to(device)\n",
    "    preds = model(device, traindata)\n",
    "\n",
    "    ##Split up into samples\n",
    "    keys = traindata.keys()\n",
    "    sample1 = {k: [] for k in keys}\n",
    "    sample2 = {k: [] for k in keys}\n",
    "    sample3 = {k: [] for k in keys}\n",
    "    sample4 = {k: [] for k in keys}\n",
    "    sample5 = {k: [] for k in keys}\n",
    "    for k in keys:\n",
    "        sample1[k] = traindata[k][0].unsqueeze(0)\n",
    "        sample2[k] = traindata[k][1].unsqueeze(0)\n",
    "        sample3[k] = traindata[k][2].unsqueeze(0)\n",
    "        sample4[k] = traindata[k][3].unsqueeze(0)\n",
    "        sample5[k] = traindata[k][4].unsqueeze(0)\n",
    "\n",
    "    ## Run on Each Sample\n",
    "    preds1 = model(device, sample1)\n",
    "    preds2 = model(device, sample2)\n",
    "    preds3 = model(device, sample3)\n",
    "    preds4 = model(device, sample4)\n",
    "    preds5 = model(device, sample5)\n",
    "    split_preds = torch.cat((preds1, preds2, preds3, preds4, preds5), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a7aea-fee7-4e8b-9526-f23ec14295e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_preds == preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62c2df-64fe-49f6-bbfe-cc4cbd79f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csiro",
   "language": "python",
   "name": "csiro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
